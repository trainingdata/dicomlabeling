---
title: "Complete Feature Set"
excerpt: ""
---
At TrainingData.io, we believe quality of the machine learning model is dependent on the quality of labelling. We empower data-scientists to control the quality of data labelling in the following ways:
[block:api-header]
{
  "title": "Segmentation and Classification"
}
[/block]
TD.io's application supports following types of annotation work:
1. Segmentation only
2. Classification only
3. Segmentation with Classification.
[block:api-header]
{
  "title": "Pixel Accurate Tools"
}
[/block]
We use advanced image processing to provide pixel accurate tools like
1. [Superpixel-segmentation](https://trainingdata.readme.io/docs/segmentation-tool) with brush and eraser,
2. [Polygon](https://trainingdata.readme.io/docs/polygon-tool) with advanced editing, bounding box, 
3. [Freehand-draw with sculpter](https://trainingdata.readme.io/docs/freehand-draw), and
4. Region of Interest (ROI) growth tool.
[block:api-header]
{
  "title": "AI Assisted Annotation"
}
[/block]
1. AI Assisted Annotation using Data-scientist's ML model
2. AI Assisted Annotation using NVIDIA Clara

[block:api-header]
{
  "title": "Data Security and Privacy Controls"
}
[/block]
On-premises hosting of datasets and annotation tools using Docker and VPN.
[block:api-header]
{
  "title": "Collaboration: Annotator's Performance Management"
}
[/block]
1. Measure, record and analyze annotator's performance on every individual task, asset and label.
2. Compare performance of multiple annotators on same task.
3. Distribute labeling work among multiple labelers and observe consensus among their work.
4. Seed annotation tasks with golden data set. Report performance of annotator on golden data set.
[block:api-header]
{
  "title": "Labeling Instruction Builder"
}
[/block]
[Labeling instruction builder](https://trainingdata.readme.io/docs/define-json) empowers data scientist to define user experience of annotators in fine-detail:
1. Labeling classes and ontology can be defined in fine-detail.
2. WYSIWYG instruction builder enables  data-scientist to view how labeling interface appears to the annotator.

We believe the quality of work performed by annotators depends on the quality of user experience in annotation tools. We build intuitive user experience for annotators.
[block:api-header]
{
  "title": "WorkForce: Humans-in-the-Loop"
}
[/block]
We partner with highest quality workforce from around the world.